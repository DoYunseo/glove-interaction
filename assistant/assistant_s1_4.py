import base64
import cv2
import numpy as np
import sounddevice as sd
import soundfile as sf
import openai
import os
import torch
import keyboard
import time
from dotenv import load_dotenv
from faster_whisper import WhisperModel
from openai import OpenAI
from pupil_labs.realtime_api.simple import discover_one_device  # â˜… eye tracker

load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI()
stt_model = WhisperModel("base", device="cuda" if torch.cuda.is_available() else "cpu")

# -------------------- ìœ í‹¸ --------------------
def timestamped_fname(prefix="base", ext="jpg"):
    t = time.localtime()
    return f"{prefix}_{t.tm_year:04d}{t.tm_mon:02d}{t.tm_mday:02d}_{t.tm_hour:02d}{t.tm_min:02d}{t.tm_sec:02d}.{ext}"

def image_to_b64(path):
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

# -------------------- Eye tracker ì—°ê²° --------------------
def connect_eye_tracker(max_wait_sec=10):
    print("ğŸ” Eye tracker íƒìƒ‰ ì¤‘...")
    device = discover_one_device(max_search_duration_seconds=max_wait_sec)
    if device is None:
        print("âŒ Eye trackerë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê°™ì€ ë„¤íŠ¸ì›Œí¬ì¸ì§€ì™€ Companion ì•± ì‹¤í–‰ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # í•„ìš”ì‹œ: IPë¡œ ì§ì ‘ ì—°ê²° ê°€ëŠ¥ (ì˜ˆì‹œ)
        # from pupil_labs.realtime_api.simple import Device
        # device = Device(address="192.168.1.169", port=8080)
        # print("IPë¡œ ì§ì ‘ ì—°ê²° ì‹œë„:", device)
    else:
        print(f"âœ… ì—°ê²°ë¨: {device}")
    return device

def capture_base_image_from_et(device):
    """
    Eye trackerì˜ ì”¬ ì¹´ë©”ë¼ì—ì„œ í•œ í”„ë ˆì„ì„ ë°›ì•„ íŒŒì¼ë¡œ ì €ì¥.
    output/base/YYYYMMDD_HHMMSS.png
    """
    if device is None:
        print("âŒ eye tracker ë¯¸ì—°ê²° ìƒíƒœì…ë‹ˆë‹¤.")
        return None

    try:
        bgr_pixels, frame_datetime = device.receive_scene_video_frame()  # Pupil Labs API
    except Exception as e:
        print("âŒ ì”¬ ì¹´ë©”ë¼ í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨:", e)
        return None

    if bgr_pixels is None:
        print("âŒ ë¹ˆ í”„ë ˆì„")
        return None

    # ì €ì¥ í´ë” ì¤€ë¹„
    os.makedirs("output/base", exist_ok=True)

    # íŒŒì¼ëª…: ë…„ì›”ì¼_ì‹œë¶„ì´ˆ.jpg
    t = time.localtime()
    fname = f"{t.tm_year:04d}{t.tm_mon:02d}{t.tm_mday:02d}_{t.tm_hour:02d}{t.tm_min:02d}{t.tm_sec:02d}.jpg"
    full_path = os.path.join("output/base", fname)

    cv2.imwrite(full_path, bgr_pixels)
    print(f"ğŸ“· ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì €ì¥: {full_path} (timestamp: {frame_datetime})")
    return full_path


# -------------------- ê³µì••ì¥ê°‘ ì œì–´ --------------------
def trigger_glove(pose: str):
    if pose == "thumbs_up":
        print("ğŸ§¤ ê³µì•• ì¥ê°‘: ìœ„ë¡œ ë”°ë´‰ í¬ì¦ˆ ì‹¤í–‰")
    elif pose == "fist":
        print("ğŸ§¤ ê³µì•• ì¥ê°‘: ì£¼ë¨¹ í¬ì¦ˆ ì‹¤í–‰")
    else:
        print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” í¬ì¦ˆ")

# -------------------- ë…¹ìŒ + STT --------------------
def record_and_transcribe_with_spacebar(sample_rate=16000, mic_device=1, et_device=None):
    print("ìŠ¤í˜ì´ìŠ¤ë°”ë¥¼ ëˆŒëŸ¬ ë…¹ìŒì„ ì‹œì‘í•˜ê³ , ë‹¤ì‹œ ëˆ„ë¥´ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    keyboard.wait('space')  # ì‹œì‘ í‚¤ ì…ë ¥ ëŒ€ê¸° (í‚¤ë‹¤ìš´ ì´ë²¤íŠ¸)
    time.sleep(0.02)        # ë°”ìš´ìŠ¤ ë°©ì§€

    # ìŠ¤í˜ì´ìŠ¤ë°”ë¥¼ ëˆ„ë¥¸ 'ê·¸ ìˆœê°„'ì˜ ë² ì´ìŠ¤ ì´ë¯¸ì§€ ìº¡ì²˜ (eye tracker)
    base_img_path = capture_base_image_from_et(et_device)

    # ìŠ¤í˜ì´ìŠ¤ë°”ì—ì„œ ì†ì„ ë—„ ë•Œê¹Œì§€ ì ê¹ ëŒ€ê¸° (ì¦‰ì‹œ ì¢…ë£Œ ë°©ì§€)
    while keyboard.is_pressed('space'):
        pass

    print("ğŸ¤ ë…¹ìŒ ì‹œì‘...")
    audio = []
    block_size = 1024
    stream = sd.InputStream(samplerate=sample_rate, channels=1, device=mic_device)
    stream.start()

    frame = np.zeros((300, 600, 3), dtype=np.uint8)
    cv2.namedWindow("Voice Meter")

    while True:
        block, _ = stream.read(block_size)
        audio.append(block)

        volume = int(np.linalg.norm(block) * 500)
        volume = min(volume, 500)

        frame[:] = 0
        cv2.rectangle(frame, (50, 250), (50 + volume, 270), (0, 255, 0), -1)
        cv2.putText(frame, "Voice Input Volume", (50, 240),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 1)
        cv2.imshow("Voice Meter", frame)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            print("ë…¹ìŒ ì·¨ì†Œë¨")
            stream.stop()
            cv2.destroyWindow("Voice Meter")
            return None, None, None

        if keyboard.is_pressed('space'):
            break

    stream.stop()
    cv2.destroyWindow("Voice Meter")

    audio_np = np.concatenate(audio, axis=0)
    wav_path = "temp.wav"
    sf.write(wav_path, audio_np, sample_rate)
    segments, _ = stt_model.transcribe(wav_path)
    text = " ".join([seg.text for seg in segments]).strip()
    print("ğŸ“„ ì¸ì‹ëœ ìŒì„±:", text if text else "(ë¹ˆ í…ìŠ¤íŠ¸)")
    return text, wav_path, base_img_path

# -------------------- VLM ì¿¼ë¦¬ --------------------
def query_vlm_with_image_and_text(text, image_path):
    """
    ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆìš”' í•œ ë‹¨ì–´ë§Œ.
    ì´ë¯¸ì§€(eye tracker ì”¬ í”„ë ˆì„) + STT í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì „ë‹¬.
    """
    if image_path is None:
        messages = [
            {"role": "system",
             "content": "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒë‹¨í•˜ëŠ” VLMì´ì•¼. í•œêµ­ì–´ë¡œ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆìš”' í•œ ë‹¨ì–´ë§Œ ë‹µí•´."},
            {"role": "user",
             "content": f"ë‹¤ìŒ ë°œí™”ê°€ ê¸ì •ì¸ì§€ íŒë‹¨í•´ì„œ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆìš”' ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•´.\në°œí™”: {text}"}
        ]
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=5,
            temperature=0
        )
        return resp.choices[0].message.content.strip()

    img_b64 = image_to_b64(image_path)
    user_content = [
        {"type": "text",
         "text": (
             "ë‹¤ìŒ ì •ë³´ë¥¼ í•¨ê»˜ ê³ ë ¤í•´ì„œ ì‚¬ìš©ìì˜ ì˜ë„ê°€ ê¸ì •ì¸ì§€ íŒë³„í•˜ë¼.\n"
             "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆìš”' í•œ ë‹¨ì–´ë§Œ ë‹µí•´.\n\n"
             f"ìŒì„± ì „ì‚¬ í…ìŠ¤íŠ¸: {text if text else '(ë¹„ì–´ ìˆìŒ)'}"
         )},
        {"type": "image_url",
         "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
    ]
    messages = [
        {"role": "system",
         "content": "ë„ˆëŠ” ì‚¬ìš©ì ì˜ë„ë¥¼ íŒë‹¨í•˜ëŠ” VLMì´ì•¼. ì¶œë ¥ì€ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆìš”'ë§Œ í—ˆìš©ëœë‹¤."},
        {"role": "user", "content": user_content}
    ]
    resp = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        max_tokens=5,
        temperature=0
    )
    return resp.choices[0].message.content.strip()

# -------------------- ì‘ë‹µâ†’í¬ì¦ˆ ë§¤í•‘ --------------------
def interpret_pose_kor(response_text):
    # 'ì˜ˆ'ë©´ thumbs_up, ê·¸ ì™¸ ì „ë¶€ fist
    norm = response_text.strip().lower()
    if norm in {"ì˜ˆ", "ë„¤"} or "ì˜ˆ" in response_text or "yes" in response_text:
        return "thumbs_up"
    return "fist"

# -------------------- ë©”ì¸ --------------------
def main():
    et_device = connect_eye_tracker(max_wait_sec=10)
    if et_device is None:
        print("âŒ Eye tracker ì—°ê²° ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    try:
        while True:
            print("\n[1] ë§ˆì´í¬ë¡œ ë§í•˜ê¸° (ì´ë¯¸ì§€+ìŒì„±)\n[2] í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸)\n[q] ì¢…ë£Œ")
            choice = input("ğŸ‘‰ ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”: ").strip().lower()

            if choice == "q":
                print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if choice == "1":
                user_text, wav_path, base_img_path = record_and_transcribe_with_spacebar(
                    sample_rate=16000, mic_device=1, et_device=et_device
                )
                if user_text is None and base_img_path is None:
                    continue
            elif choice == "2":
                base_img_path = capture_base_image_from_et(et_device)
                user_text = input("âœï¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if not user_text and base_img_path is None:
                    print("âŒ ì…ë ¥ì´ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ë„ ì—†ìŒ.")
                    continue
            else:
                print("â— ìœ íš¨í•œ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”.")
                continue

            vlm_resp = query_vlm_with_image_and_text(user_text, base_img_path)
            print("ğŸ§  VLM ì‘ë‹µ(ì›ë¬¸):", vlm_resp)

            pose = interpret_pose_kor(vlm_resp)
            trigger_glove(pose)
    finally:
        print("ğŸ”Œ Eye tracker ì—°ê²° ì¢…ë£Œ")
        try:
            et_device.close()
        except Exception:
            pass

if __name__ == "__main__":
    main()
